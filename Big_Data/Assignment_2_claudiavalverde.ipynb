{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31682f9a-4945-4feb-bc89-83340720aa3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "#### First Name: Clàudia\n",
    "#### Last Name: Valverde Sanchez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8a93105-2c23-4b4e-9362-e9c2aea7f293",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Load Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8dbea9-daf7-4134-b06d-9c414621621b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure to upload the corona_tweet_new.json file\n",
    "df_twitter = spark.read.json(\"/FileStore/tables/corona_tweet_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32afcf7e-c82b-4c27-91e3-028eb7f11302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- created_at: string (nullable = true)\n |-- favorite_count: long (nullable = true)\n |-- hashtags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- id: string (nullable = true)\n |-- in_reply_to_status_id: string (nullable = true)\n |-- in_reply_to_user_id_str: string (nullable = true)\n |-- location: string (nullable = true)\n |-- reply_count: long (nullable = true)\n |-- retweet_count: long (nullable = true)\n |-- source: string (nullable = true)\n |-- user: struct (nullable = true)\n |    |-- contributors_enabled: boolean (nullable = true)\n |    |-- created_at: string (nullable = true)\n |    |-- default_profile: boolean (nullable = true)\n |    |-- default_profile_image: boolean (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- favourites_count: long (nullable = true)\n |    |-- follow_request_sent: string (nullable = true)\n |    |-- followers_count: long (nullable = true)\n |    |-- following: string (nullable = true)\n |    |-- friends_count: long (nullable = true)\n |    |-- geo_enabled: boolean (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- id_str: string (nullable = true)\n |    |-- is_translator: boolean (nullable = true)\n |    |-- lang: string (nullable = true)\n |    |-- listed_count: long (nullable = true)\n |    |-- location: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- notifications: string (nullable = true)\n |    |-- profile_background_color: string (nullable = true)\n |    |-- profile_background_image_url: string (nullable = true)\n |    |-- profile_background_image_url_https: string (nullable = true)\n |    |-- profile_background_tile: boolean (nullable = true)\n |    |-- profile_banner_url: string (nullable = true)\n |    |-- profile_image_url: string (nullable = true)\n |    |-- profile_image_url_https: string (nullable = true)\n |    |-- profile_link_color: string (nullable = true)\n |    |-- profile_sidebar_border_color: string (nullable = true)\n |    |-- profile_sidebar_fill_color: string (nullable = true)\n |    |-- profile_text_color: string (nullable = true)\n |    |-- profile_use_background_image: boolean (nullable = true)\n |    |-- protected: boolean (nullable = true)\n |    |-- screen_name: string (nullable = true)\n |    |-- statuses_count: long (nullable = true)\n |    |-- time_zone: string (nullable = true)\n |    |-- translator_type: string (nullable = true)\n |    |-- url: string (nullable = true)\n |    |-- utc_offset: string (nullable = true)\n |    |-- verified: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2460671-b6a6-47bd-84ba-e63c00bd541a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+--------+-----------+-------------+--------------------+-------------------+--------------------+------------------+--------------------+\n|          created_at|favorite_count|            hashtags|                 id|in_reply_to_status_id|in_reply_to_user_id_str|location|reply_count|retweet_count|              source|        user_id_str|user_followers_count|user_friends_count|     user_created_at|\n+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+--------+-----------+-------------+--------------------+-------------------+--------------------+------------------+--------------------+\n|Mon Apr 20 15:01:...|          5545|                  []|1252251164227362821|                 null|                   null|   India|       3460|         5477|<a href=\"https://...|           93612837|                 121|               759|Mon Nov 30 11:38:...|\n|Mon Apr 20 15:01:...|          7682|                  []|1252251164256555009|                 null|                   null|      UK|        418|         6513|<a href=\"http://t...|          346443880|                 208|              1196|Mon Aug 01 08:15:...|\n|Mon Apr 20 15:01:...|          2775|                  []|1252251166504824833|                 null|                   null|   India|       9965|         6374|<a href=\"http://t...|         1154311994|                4790|               574|Wed Feb 06 15:26:...|\n|Mon Apr 20 15:01:...|           856|                  []|1252251166655647744|                 null|                   null|   Spain|       9931|         4307|<a href=\"http://t...|1244090006408327169|                   6|                73|Sun Mar 29 02:32:...|\n|Mon Apr 20 15:01:...|          7705|                  []|1252251167884795905|                 null|                   null|   Spain|       4178|         1912|<a href=\"http://t...|         2781749028|                  29|               367|Sun Aug 31 07:04:...|\n|Mon Apr 20 15:01:...|          5209|                  []|1252251170057457665|                 null|                   null|     USA|       6258|         7927|<a href=\"https://...|         1297207694|                9069|              9109|Mon Mar 25 01:08:...|\n|Mon Apr 20 15:01:...|          9486|                  []|1252251170845986816|                 null|                   null|      UK|       9506|         5558|<a href=\"http://t...|1247795796893814784|                   2|                24|Wed Apr 08 08:00:...|\n|Mon Apr 20 15:01:...|          1946|                  []|1252251171252772867|                 null|                   null|  Canada|       3830|         5341|<a href=\"http://t...|         2258446142|                 252|               144|Mon Dec 23 04:17:...|\n|Mon Apr 20 15:01:...|          5621|                  []|1252251171697410048|                 null|                   null|   China|         93|         6818|<a href=\"http://t...|          549592791|               20192|              4897|Mon Apr 09 20:07:...|\n|Mon Apr 20 15:01:...|          5272|                  []|1252251172519448576|                 null|                   null|Pakistan|       9930|          681|<a href=\"http://t...| 816782950855491585|                 112|               361|Wed Jan 04 23:06:...|\n|Mon Apr 20 15:01:...|          7056|                  []|1252251172582416384|                 null|                   null|   Spain|       9254|         4883|<a href=\"http://t...|          426476518|                 406|               479|Fri Dec 02 08:46:...|\n|Mon Apr 20 15:02:...|          7312|                  []|1252251172997443585|                 null|                   null|   China|       6928|         7067|<a href=\"https://...|          127050602|                7875|              8297|Sat Mar 27 22:03:...|\n|Mon Apr 20 15:02:...|          2001|                  []|1252251174448726017|                 null|                   null| Germany|       6602|         6924|<a href=\"http://t...| 929673345875841025|                   7|               229|Sun Nov 12 11:32:...|\n|Mon Apr 20 15:02:...|           255|                  []|1252251174503358468|                 null|                   null|   India|       1174|          597|<a href=\"https://...|         1433865350|                   6|                94|Thu May 16 19:43:...|\n|Mon Apr 20 15:01:...|          3690|                  []|1252251172527845379|                 null|                   null|   India|       6089|         6735|<a href=\"http://t...|          238382848|                2840|               403|Sat Jan 15 01:45:...|\n|Mon Apr 20 15:02:...|          2620|[boxing, kickboxi...|1252251175547592709|                 null|                   null|  Mexico|       1576|         2357|<a href=\"http://t...|1209683044832931840|                  20|                47|Wed Dec 25 03:51:...|\n|Mon Apr 20 15:02:...|          2576|                  []|1252251175769997312|                 null|                   null|  Mexico|       2552|         5827|<a href=\"http://t...|         1488737106|                  33|               383|Thu Jun 06 21:22:...|\n|Mon Apr 20 15:02:...|           893|                  []|1252251175866568705|                 null|                   null|   China|        248|         8969|<a href=\"http://t...|1249239739384946690|                  27|                65|Sun Apr 12 07:35:...|\n|Mon Apr 20 15:02:...|          8219|                  []|1252251176109780994|                 null|                   null|   India|        724|         9712|<a href=\"http://t...|         3298265327|                 389|               729|Mon May 25 18:41:...|\n|Mon Apr 20 15:02:...|          7205|                  []|1252251177103888390|                 null|                   null|      UK|       4747|         1222|<a href=\"http://t...|1159501970438119424|                 892|                15|Thu Aug 08 16:29:...|\n+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+--------+-----------+-------------+--------------------+-------------------+--------------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "### From the user nested col select the following cols only id_str,followers_count,friends_count and created at \n",
    "# (2 points)\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "# Select the specific nested columns\n",
    "\n",
    "df_twitter=df_twitter.select(\n",
    "    col(\"created_at\"),\n",
    "    col(\"favorite_count\"),\n",
    "    col(\"hashtags\"),\n",
    "    col(\"id\"),\n",
    "    col(\"in_reply_to_status_id\"),\n",
    "    col(\"in_reply_to_user_id_str\"),\n",
    "    col(\"location\"),\n",
    "    col(\"reply_count\"),\n",
    "    col(\"retweet_count\"),\n",
    "    col(\"source\"),\n",
    "    col(\"user.id_str\").alias(\"user_id_str\"),\n",
    "    col(\"user.followers_count\").alias(\"user_followers_count\"),\n",
    "    col(\"user.friends_count\").alias(\"user_friends_count\"),\n",
    "    col(\"user.created_at\").alias(\"user_created_at\")\n",
    ")\n",
    "\n",
    "# Show the selected columns\n",
    "df_twitter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b471703-d40c-4378-b313-09ada92d7916",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- created_at: string (nullable = true)\n |-- favorite_count: long (nullable = true)\n |-- hashtags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- id: string (nullable = true)\n |-- in_reply_to_status_id: string (nullable = true)\n |-- in_reply_to_user_id_str: string (nullable = true)\n |-- location: string (nullable = true)\n |-- reply_count: long (nullable = true)\n |-- retweet_count: long (nullable = true)\n |-- source: string (nullable = true)\n |-- user_id_str: string (nullable = true)\n |-- user_followers_count: long (nullable = true)\n |-- user_friends_count: long (nullable = true)\n |-- user_created_at: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5404794-ac4a-40d0-920d-15415f23f764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 15894"
     ]
    }
   ],
   "source": [
    "# Print the total count of number of records in df_twitter(1 point)\n",
    "df_twitter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b937f2f-1ee5-4f6b-82fc-dd26c80c1318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n|   extracted_source|              source|\n+-------------------+--------------------+\n|    Twitter Web App|<a href=\"https://...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n| Twitter Web Client|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n|Twitter for Android|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n+-------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Extract the source lable from source col by droping the anchor tab and save it as another col named extracted_source\n",
    "# for example <a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a> => Twitter Web App\n",
    "# you can use \"<a [^>]+>([^<]+)\" as regualr expresion and the group would be 1 for this regular expression.\n",
    "#(4 points)\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Extract the source label from the source column and save it as another column named extracted_source\n",
    "df_twitter = df_twitter.withColumn(\n",
    "    \"extracted_source\",\n",
    "    regexp_extract(col(\"source\"), r'<a [^>]+>([^<]+)</a>', 1)\n",
    ")\n",
    "df_twitter.select(col('extracted_source'),col('source')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc2a2ab-95df-4a32-afab-bc1519176136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame into RDD\n",
    "rdd_twitter=df_twitter.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe54b15-11ee-451e-ad62-188a3537fe85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------+-------------------+---------------------+-----------------------+--------+-----------+-------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+\n|          created_at|favorite_count|hashtags|                 id|in_reply_to_status_id|in_reply_to_user_id_str|location|reply_count|retweet_count|              source|        user_id_str|user_followers_count|user_friends_count|     user_created_at|   extracted_source|\n+--------------------+--------------+--------+-------------------+---------------------+-----------------------+--------+-----------+-------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+\n|Mon Apr 20 15:01:...|          5545|      []|1252251164227362821|                 null|                   null|   India|       3460|         5477|<a href=\"https://...|           93612837|                 121|               759|Mon Nov 30 11:38:...|    Twitter Web App|\n|Mon Apr 20 15:01:...|          7682|      []|1252251164256555009|                 null|                   null|      UK|        418|         6513|<a href=\"http://t...|          346443880|                 208|              1196|Mon Aug 01 08:15:...|Twitter for Android|\n|Mon Apr 20 15:01:...|          2775|      []|1252251166504824833|                 null|                   null|   India|       9965|         6374|<a href=\"http://t...|         1154311994|                4790|               574|Wed Feb 06 15:26:...|Twitter for Android|\n|Mon Apr 20 15:01:...|           856|      []|1252251166655647744|                 null|                   null|   Spain|       9931|         4307|<a href=\"http://t...|1244090006408327169|                   6|                73|Sun Mar 29 02:32:...|Twitter for Android|\n|Mon Apr 20 15:01:...|          7705|      []|1252251167884795905|                 null|                   null|   Spain|       4178|         1912|<a href=\"http://t...|         2781749028|                  29|               367|Sun Aug 31 07:04:...|Twitter for Android|\n|Mon Apr 20 15:01:...|          5209|      []|1252251170057457665|                 null|                   null|     USA|       6258|         7927|<a href=\"https://...|         1297207694|                9069|              9109|Mon Mar 25 01:08:...|    Twitter Web App|\n|Mon Apr 20 15:01:...|          9486|      []|1252251170845986816|                 null|                   null|      UK|       9506|         5558|<a href=\"http://t...|1247795796893814784|                   2|                24|Wed Apr 08 08:00:...| Twitter Web Client|\n|Mon Apr 20 15:01:...|          1946|      []|1252251171252772867|                 null|                   null|  Canada|       3830|         5341|<a href=\"http://t...|         2258446142|                 252|               144|Mon Dec 23 04:17:...|Twitter for Android|\n|Mon Apr 20 15:01:...|          5621|      []|1252251171697410048|                 null|                   null|   China|         93|         6818|<a href=\"http://t...|          549592791|               20192|              4897|Mon Apr 09 20:07:...|Twitter for Android|\n|Mon Apr 20 15:01:...|          5272|      []|1252251172519448576|                 null|                   null|Pakistan|       9930|          681|<a href=\"http://t...| 816782950855491585|                 112|               361|Wed Jan 04 23:06:...| Twitter for iPhone|\n+--------------------+--------------+--------+-------------------+---------------------+-----------------------+--------+-----------+-------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a temporay table in memory with name as twitter (1 point)\n",
    "\n",
    "#createOrReplaceTempView method. This method registers the DataFrame as a temporary table, which you can then query using SQL.\n",
    "df_twitter.createOrReplaceTempView(\"twitter\")\n",
    "\n",
    "\n",
    "# to check\n",
    "result = spark.sql(\"SELECT * FROM twitter LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3bf085-81dd-4900-b4da-d8992988eef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Analyze Data\n",
    "\n",
    "#### You will be writing code to find the answer to the questions listed below using Just RDD, Using spark SQL \n",
    "\n",
    "- Analyze using RDD \n",
    "- Analyze using Dataframe without temp table \n",
    "- Analyze using spark.sql with temp table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3718e4f-b9bb-47d4-bcdb-1b95060ea597",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Get total number of unique users (1 point for each type)\n",
    "\n",
    "**My reasoning:** Selecting the user name column (user_id_str), count the number of unique (distinct) user names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b728e4-d5b3-4426-a720-68dbd41d592e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of column user_id_str:\n93612837\n346443880\n1154311994\n1244090006408327169\n2781749028\n1297207694\n1247795796893814784\n2258446142\n549592791\n816782950855491585\n\nTotal number of unique users: 14094\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "\n",
    "'''\n",
    "Knowing user_id_str corresponds to column 10 of the original df, we select it and get the unique users\n",
    "'''\n",
    "\n",
    "# checking if I'm selecting the correct column\n",
    "selected_column = rdd_twitter.map(lambda tweet: tweet[10]).take(10)\n",
    "print(\"Sample of column user_id_str:\")\n",
    "for value in selected_column:\n",
    "    print(value)\n",
    "  \n",
    "\n",
    "unique_users = rdd_twitter.map(lambda tweet: tweet[10]).distinct().count()\n",
    "print(f\"\\nTotal number of unique users: {unique_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8811c4f1-b7e9-402c-b173-d911d3fbd30b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users: 14094\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "\n",
    "# Get the total number of unique users\n",
    "unique_users_count = df_twitter.select(\"user_id_str\").distinct().count()\n",
    "\n",
    "print(f\"Total number of unique users: {unique_users_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166afb53-4bf9-4188-8e34-4d6860be63d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users: 14094\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "\n",
    "# Use Spark SQL to get the total number of unique users\n",
    "unique_users_count = spark.sql(\"SELECT COUNT(DISTINCT user_id_str) AS unique_users FROM twitter\").collect()[0]['unique_users']\n",
    "\n",
    "print(f\"Total number of unique users: {unique_users_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026629d1-5215-40c7-a274-33514f261eef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Get count of user who have more than 1 tweet in the data (2 points)\n",
    "\n",
    "\n",
    "**My reasoning:** In order to tackle the exercise, first I need to know the number of tweets each user has done, after that, select only those users who have done more than 1 tweet and finally count how many fulfill all these conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2088c395-da3a-409a-8c94-8fa678d3593c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who have more than one tweet: 1016\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "\n",
    "# Map each tweet to the user ID\n",
    "user_ids = rdd_twitter.map(lambda tweet: (tweet[10], 1)) # desired column is in position 10\n",
    "\n",
    "# Count the occurrences of each user ID\n",
    "user_tweet_counts = user_ids.countByKey()\n",
    "\n",
    "# Filter users with more than one tweet\n",
    "users_with_multiple_tweets = list(filter(lambda x: x[1] > 1, user_tweet_counts.items()))\n",
    "\n",
    "# Count the number of filtered users\n",
    "count_users_with_multiple_tweets = len(users_with_multiple_tweets)\n",
    "\n",
    "print(\"Number of users who have more than one tweet:\", count_users_with_multiple_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71465c1d-d375-4052-adbe-e95dd671b955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of users who have more than one tweet: 1016\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "\n",
    "# Group by user_id_str and count the number of tweets for each user\n",
    "count_users_with_multiple_tweets = df_twitter.groupby(\"user_id_str\").count().filter(\"count>1\").count()\n",
    "\n",
    "print(\"Count of users who have more than one tweet:\", count_users_with_multiple_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5e04a7-13da-4c8f-b2fc-6af9a3791722",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of users who have more than one tweet: 1016\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "\n",
    "# Count the number of tweets for each user and filter users with more than one tweet\n",
    "sql_query = \"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM (SELECT COUNT(user_id_str) AS tweet_count FROM twitter GROUP BY user_id_str)\n",
    "    WHERE tweet_count > 1 \n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "users_with_multiple_tweets_sql = spark.sql(sql_query).collect()[0]['count(1)']\n",
    "\n",
    "print(\"Count of users who have more than one tweet:\", users_with_multiple_tweets_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a6d8f82-ab2b-4bc9-b266-b2486624f2de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Get total number unique extracted_source (1 point each)\n",
    "\n",
    "**My reasoning:** First filter by the column corresponding to extracted_source (14) and then count the number of unique (distinct) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcdc745-55f2-4e9c-84d7-267086ba437c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of column extracted_source:\nTwitter Web App\nTwitter for Android\nTwitter for Android\nTwitter for Android\nTwitter for Android\nTwitter Web App\nTwitter Web Client\nTwitter for Android\nTwitter for Android\nTwitter for iPhone\n\nTotal number of unique extracted_source: 133\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "\n",
    "# checking if I'm selecting the correct column\n",
    "selected_column = rdd_twitter.map(lambda tweet: tweet[14]).take(10)\n",
    "print(\"Sample of column extracted_source:\")\n",
    "for value in selected_column:\n",
    "    print(value)\n",
    "  \n",
    "unique_extracted_source = rdd_twitter.map(lambda tweet: tweet[14]).distinct().count()\n",
    "print(f\"\\nTotal number of unique extracted_source: {unique_extracted_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9974fd-f96e-4b8c-97c8-fcbae505dd91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique extracted_source: 133\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "\n",
    "# Get the total number of unique users\n",
    "unique_extracted_source_count = df_twitter.select(\"extracted_source\").distinct().count()\n",
    "\n",
    "print(f\"Total number of unique extracted_source: {unique_extracted_source_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f436e2ae-19ba-46ba-825c-18d97c5f30bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique extracted_source: 133\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "\n",
    "# Get the total number of unique extracted_source\n",
    "sql_query = \"\"\"\n",
    "    SELECT COUNT(DISTINCT extracted_source) AS unique_extracted_source \n",
    "    FROM twitter\n",
    "\"\"\"\n",
    "unique_extracted_source_count = spark.sql(sql_query).collect()[0]['unique_extracted_source']\n",
    "\n",
    "print(f\"Total number of unique extracted_source: {unique_extracted_source_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "188bdf59-cf92-4d08-927b-f003e0098ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Get top 5 most used extracted_source\n",
    "\n",
    "**My reasoning:** In order to tackle the exercise, first I need to know how many times each extracted_source appears, after that, select the top 5 entries in number of extracted_source.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07037b6-c226-4de2-b942-da6f732b20ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_source: Twitter for Android source_count: 6262\nextracted_source: Twitter for iPhone source_count: 5698\nextracted_source: Twitter Web App source_count: 2878\nextracted_source: Twitter for iPad source_count: 428\nextracted_source: Twitter Web Client source_count: 136\n"
     ]
    }
   ],
   "source": [
    "# Using RDD (5 points)\n",
    "\n",
    "# Map each row of the DataFrame to extract the extracted_source value\n",
    "rdd_extracted_sources = rdd_twitter.map(lambda row: row[14])\n",
    "\n",
    "# Filter out null extracted_source values (not really necessary)\n",
    "rdd_filtered_sources = rdd_extracted_sources.filter(lambda source: source is not None)\n",
    "\n",
    "# Count the occurrences of each extracted_source\n",
    "rdd_source_counts = rdd_filtered_sources.map(lambda source: (source, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort the results in descending order by the count\n",
    "rdd_sorted_sources = rdd_source_counts.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Take the top 5 rows\n",
    "top_5_sources = rdd_sorted_sources.take(5)\n",
    "\n",
    "# Print the top 5 most used extracted_source values\n",
    "for source, count in top_5_sources:\n",
    "    print(\"extracted_source:\", source, \"source_count:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8175f756-d354-46b9-8ecb-24798c606293",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|   extracted_source|count|\n+-------------------+-----+\n|Twitter for Android| 6262|\n| Twitter for iPhone| 5698|\n|    Twitter Web App| 2878|\n|   Twitter for iPad|  428|\n| Twitter Web Client|  136|\n+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame (2 points)\n",
    "\n",
    "df_filtered_sources = df_twitter.groupby('extracted_source').count().sort(\"count\", ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff8fab4-c6b3-4dec-ba2e-525ef27eb304",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n|   extracted_source|source_count|\n+-------------------+------------+\n|Twitter for Android|        6262|\n| Twitter for iPhone|        5698|\n|    Twitter Web App|        2878|\n|   Twitter for iPad|         428|\n| Twitter Web Client|         136|\n+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table. (2 points)\n",
    "\n",
    "# Get the top 5 most used extracted_source values\n",
    "sql_query = \"\"\"\n",
    "    SELECT extracted_source, COUNT(*) AS source_count\n",
    "    FROM twitter\n",
    "    WHERE extracted_source IS NOT NULL\n",
    "    GROUP BY extracted_source\n",
    "    ORDER BY source_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "top_5_sources_df = spark.sql(sql_query)\n",
    "\n",
    "# Show the top 5 most used extracted_source values\n",
    "top_5_sources_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b302336-03bd-448f-9608-a63b4ef1daa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.5 Get count of distinct hashtags used (5 point each) \n",
    "\n",
    "**My reasoning:** From hashtags column (2), get the unique (distinct) hashtags and count how many there are.\n",
    "\n",
    "Hashtags columns has a different structure than the rest of the column, each value is a kind of list type. So will have to use 'explode' or 'flatMap' on the column for the different procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945573b7-aaa7-44d0-8086-4230a8f81d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct hashtags count: 1215\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "\n",
    "# Explode the RDD to create one row per hashtag\n",
    "rdd_exploded = rdd_twitter.flatMap(lambda x: x[2]) # col2 corresponds to hastags column\n",
    "\n",
    "# Count the distinct hashtags\n",
    "distinct_hashtags_count = rdd_exploded.distinct().count()\n",
    "\n",
    "print(\"Distinct hashtags count:\", distinct_hashtags_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34af0927-8b11-4c2c-b653-aabc3b5de016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct hashtags count: 1215\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Explode the hashtags column to create one row per hashtag\n",
    "df_exploded = df_twitter.select(explode(col('hashtags')))\n",
    "\n",
    "# Count the distinct hashtags\n",
    "distinct_hashtags_count = df_exploded.select('col').distinct().count()\n",
    "\n",
    "print(\"Distinct hashtags count:\", distinct_hashtags_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de8afaa-e618-46b7-a471-786267e8da6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct hashtags count: 1215\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "\n",
    "# Get the count of distinct hashtags\n",
    "sql_query = \"\"\"\n",
    "    SELECT COUNT(DISTINCT hashtag) AS distinct_hashtags_count\n",
    "    FROM (\n",
    "        SELECT explode(hashtags) AS hashtag\n",
    "        FROM twitter\n",
    "    ) temp\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "distinct_hashtags_count_df = spark.sql(sql_query).collect()[0]['distinct_hashtags_count']\n",
    "\n",
    "# Show the count of distinct hashtags\n",
    "print(\"Distinct hashtags count:\", distinct_hashtags_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cedd1db9-f6e1-4114-8187-76e447dd568b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Get top 5 hashtags\n",
    "\n",
    "**My reasoning**: from hastaghs column, first count each hashtag how many times it appears, then order them by descending order of the this count and get the top 5 entries.\n",
    "Remember that hashtags column its still in a different format than the rest of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae2407d-566e-4071-b2be-b85930690339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hashtags:\nطبق_القدرات_للثانويه_ياريس : 385\nCorona : 319\nOilPrice : 251\nCOVID19 : 125\ncorona : 123\n"
     ]
    }
   ],
   "source": [
    "# Using RDD (4 points)\n",
    "\n",
    "# Get the count of each hashtag\n",
    "hashtag_counts = rdd_twitter.flatMap(lambda x: x[2]).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get the top 5 hashtags\n",
    "top_5_hashtags = hashtag_counts.takeOrdered(5, key=lambda x: -x[1])\n",
    "\n",
    "print(\"Top 5 hashtags:\")\n",
    "for hashtag, count in top_5_hashtags:\n",
    "    print(hashtag, \":\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739048d3-575b-49ed-a5bc-f92cd30ba224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|                 col|count|\n+--------------------+-----+\n|طبق_القدرات_للثان...|  385|\n|              Corona|  319|\n|            OilPrice|  251|\n|             COVID19|  125|\n|              corona|  123|\n+--------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame (2 points)\n",
    "\n",
    "# Explode the hashtags column to create one row per hashtag\n",
    "df_exploded = df_twitter.select(explode(col('hashtags'))).groupby('col').count().sort(\"count\", ascending = False).show(5)\n",
    "df_exploded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8357166-5055-4127-a832-1df5ac844808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n|             hashtag|hashtag_count|\n+--------------------+-------------+\n|طبق_القدرات_للثان...|          385|\n|              Corona|          319|\n|            OilPrice|          251|\n|             COVID19|          125|\n|              corona|          123|\n+--------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table. (2 points)\n",
    "\n",
    "sql_query = \"\"\"\n",
    "    SELECT hashtag, COUNT(*) AS hashtag_count\n",
    "    FROM (\n",
    "        SELECT explode(hashtags) AS hashtag\n",
    "        FROM twitter\n",
    "    ) temp\n",
    "    GROUP BY hashtag\n",
    "    ORDER BY hashtag_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "top5_hashtags_count_df = spark.sql(sql_query)\n",
    "\n",
    "top5_hashtags_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f2e235a-34d7-4c48-947b-c092387a21ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.7 Get total number of tweets which are retweeted more than 100 times\n",
    "\n",
    "**My reasoning**: \"retweet column\" (8) corresponds to the number of times each tweet has been retweeted. From this column, select (filter) those entries haigher than 100, count how many fulfill this condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ccc01a7-f5ea-48bd-9bac-8b5dfe7fba0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets retweeted more than 100 times: 15753\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "\n",
    "# knowing retweeted count is column 8\n",
    "# Filter tweets with retweet count more than 100\n",
    "retweeted_tweets = rdd_twitter.filter(lambda tweet: tweet[8] > 100)\n",
    "\n",
    "# Count the total number of retweeted tweets\n",
    "total_retweeted_tweets = retweeted_tweets.count()\n",
    "\n",
    "print(\"Total number of tweets retweeted more than 100 times:\", total_retweeted_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9502388c-f59a-4dc1-82a1-255cab5af332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets retweeted more than 100 times: 15753\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter tweets with retweet count more than 100\n",
    "retweeted_tweets_df = df_twitter.filter(col(\"retweet_count\") > 100)\n",
    "\n",
    "# Count the total number of retweeted tweets\n",
    "total_retweeted_tweets_df = retweeted_tweets_df.count()\n",
    "\n",
    "print(\"Total number of tweets retweeted more than 100 times:\", total_retweeted_tweets_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be34af63-a4e5-40ae-8f2e-dae437a0f1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets retweeted more than 100 times: 15753\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "sql_query = \"\"\"\n",
    "    SELECT COUNT(*) AS retweet_count_above_100\n",
    "    FROM twitter\n",
    "    WHERE retweet_count > 100\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "retweet_count_df = spark.sql(sql_query)\n",
    "\n",
    "# Show the result\n",
    "print(\"Total number of tweets retweeted more than 100 times:\", retweet_count_df.collect()[0]['retweet_count_above_100'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48189fee-024e-4ff5-8109-da51120e3453",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.8 Get top 3 most retweeted tweets per country (8 points)\n",
    "\n",
    "**My reasoning**: From the 'retweeted tweets' (8) column and 'location' column (3) group tweets x country, from this selection, get the descending orfer and select the top 3 tweets for each group (country). I will also  print the retweeted column for each selected tweet in order to ensure that I'm doing the correct search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b894a602-d6ee-44b5-9c4c-96c92ccf8309",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: India\n    Tweet ID: 1252332114948874240, Retweet Count: 9988\n    Tweet ID: 1252252336921206787, Retweet Count: 9976\n    Tweet ID: 1252254519116746754, Retweet Count: 9973\nCountry: Pakistan\n    Tweet ID: 1252334264248606720, Retweet Count: 9988\n    Tweet ID: 1252251912084357121, Retweet Count: 9975\n    Tweet ID: 1252252126694309888, Retweet Count: 9973\nCountry: USA\n    Tweet ID: 1252331777806524416, Retweet Count: 9994\n    Tweet ID: 1252254239805579264, Retweet Count: 9987\n    Tweet ID: 1252335464750735362, Retweet Count: 9982\nCountry: Italy\n    Tweet ID: 1252252106750377996, Retweet Count: 9994\n    Tweet ID: 1252251206027816960, Retweet Count: 9984\n    Tweet ID: 1252330500670337024, Retweet Count: 9971\nCountry: Canada\n    Tweet ID: 1252335430323888128, Retweet Count: 9997\n    Tweet ID: 1252254877939531776, Retweet Count: 9992\n    Tweet ID: 1252252082825986051, Retweet Count: 9987\nCountry: China\n    Tweet ID: 1252335780707684352, Retweet Count: 9998\n    Tweet ID: 1252253596516843520, Retweet Count: 9993\n    Tweet ID: 1252255562525560832, Retweet Count: 9984\nCountry: Chile\n    Tweet ID: 1252253612140490759, Retweet Count: 9988\n    Tweet ID: 1252334891951427585, Retweet Count: 9984\n    Tweet ID: 1252253710182481920, Retweet Count: 9978\nCountry: UK\n    Tweet ID: 1252333018578145280, Retweet Count: 9991\n    Tweet ID: 1252252091822870529, Retweet Count: 9989\n    Tweet ID: 1252254043973603329, Retweet Count: 9985\nCountry: Mexico\n    Tweet ID: 1252253843145912320, Retweet Count: 9998\n    Tweet ID: 1252255209776189442, Retweet Count: 9994\n    Tweet ID: 1252252016006422533, Retweet Count: 9971\nCountry: Spain\n    Tweet ID: 1252335445876367361, Retweet Count: 9992\n    Tweet ID: 1252334839094599681, Retweet Count: 9981\n    Tweet ID: 1252254696112300032, Retweet Count: 9969\nCountry: Germany\n    Tweet ID: 1252334028092399622, Retweet Count: 9999\n    Tweet ID: 1252330902325248000, Retweet Count: 9997\n    Tweet ID: 1252252295510855682, Retweet Count: 9990\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "\n",
    "# Map to extract the required fields (country, tweet id, retweet count)\n",
    "rdd_mapped = rdd_twitter.map(lambda row: (row[6], (row[3], row[8])))\n",
    "\n",
    "# Group by country\n",
    "rdd_grouped = rdd_mapped.groupByKey()\n",
    "\n",
    "# Sort the tweets within each group by retweet count in descending order and take the top 3\n",
    "rdd_top3_per_country = rdd_grouped.mapValues(lambda tweets: sorted(tweets, key=lambda x: x[1], reverse=True)[:3])\n",
    "\n",
    "# Collect the results\n",
    "top3_per_country = rdd_top3_per_country.collect()\n",
    "\n",
    "# Print the results\n",
    "for country, top3_tweets in top3_per_country:\n",
    "    print(f\"Country: {country}\")\n",
    "    for tweet in top3_tweets:\n",
    "        print(f\"    Tweet ID: {tweet[0]}, Retweet Count: {tweet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08dd149d-2c03-459f-b281-4488db7241bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+\n|location|id                 |retweet_count|\n+--------+-------------------+-------------+\n|Canada  |1252335430323888128|9997         |\n|Canada  |1252254877939531776|9992         |\n|Canada  |1252252082825986051|9987         |\n|Chile   |1252253612140490759|9988         |\n|Chile   |1252334891951427585|9984         |\n|Chile   |1252253710182481920|9978         |\n|China   |1252335780707684352|9998         |\n|China   |1252253596516843520|9993         |\n|China   |1252255562525560832|9984         |\n|Germany |1252334028092399622|9999         |\n|Germany |1252330902325248000|9997         |\n|Germany |1252252295510855682|9990         |\n|India   |1252332114948874240|9988         |\n|India   |1252252336921206787|9976         |\n|India   |1252254519116746754|9973         |\n|Italy   |1252252106750377996|9994         |\n|Italy   |1252251206027816960|9984         |\n|Italy   |1252330500670337024|9971         |\n|Mexico  |1252253843145912320|9998         |\n|Mexico  |1252255209776189442|9994         |\n+--------+-------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Filter out rows where the location is None (not really necessary)\n",
    "df_filtered = df_twitter.filter(col(\"location\").isNotNull())\n",
    "\n",
    "# Define window specification (for each location order descending retweet count)\n",
    "window_spec = Window.partitionBy(\"location\").orderBy(col(\"retweet_count\").desc())\n",
    "\n",
    "# Add row number within each partition (country)\n",
    "df_with_row_num = df_filtered.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get only the top 3 retweeted tweets per country\n",
    "top3_per_country_df = df_with_row_num.filter(col(\"row_number\") <= 3).drop(\"row_number\")\n",
    "\n",
    "# Show the result\n",
    "top3_per_country_df.select(\"location\", \"id\", \"retweet_count\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f26dc93-f30e-46c5-b959-dc284fc0dbc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n|country|                 id|retweet_count|\n+-------+-------------------+-------------+\n| Canada|1252335430323888128|         9997|\n| Canada|1252254877939531776|         9992|\n| Canada|1252252082825986051|         9987|\n|  Chile|1252253612140490759|         9988|\n|  Chile|1252334891951427585|         9984|\n|  Chile|1252253710182481920|         9978|\n|  China|1252335780707684352|         9998|\n|  China|1252253596516843520|         9993|\n|  China|1252255562525560832|         9984|\n|Germany|1252334028092399622|         9999|\n|Germany|1252330902325248000|         9997|\n|Germany|1252252295510855682|         9990|\n|  India|1252332114948874240|         9988|\n|  India|1252252336921206787|         9976|\n|  India|1252254519116746754|         9973|\n|  Italy|1252252106750377996|         9994|\n|  Italy|1252251206027816960|         9984|\n|  Italy|1252330500670337024|         9971|\n| Mexico|1252253843145912320|         9998|\n| Mexico|1252255209776189442|         9994|\n+-------+-------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "\n",
    "# Get the top 3 most retweeted tweets per country\n",
    "sql_query = \"\"\"\n",
    "    SELECT country, id, retweet_count\n",
    "    FROM (\n",
    "        SELECT location AS country, id, retweet_count,\n",
    "               ROW_NUMBER() OVER (PARTITION BY location ORDER BY retweet_count DESC) AS rank\n",
    "        FROM twitter\n",
    "        WHERE location IS NOT NULL\n",
    "    ) ranked\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY country, retweet_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "top_retweeted_tweets_df = spark.sql(sql_query)\n",
    "\n",
    "top_retweeted_tweets_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d29ffc84-4cf0-4588-90dc-84fc373c1855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.9 Total number of tweets per country\n",
    "\n",
    "**My reasoning**: Having the groups of tweets x countries (similar as in previous exercise), count how many tweets (not retweets) are for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74d26ec-8348-40e2-948d-5b89ae4df1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India: 1480 tweets\nPakistan: 1470 tweets\nUSA: 1539 tweets\nItaly: 1422 tweets\nCanada: 1441 tweets\nChina: 1457 tweets\nChile: 1410 tweets\nUK: 1376 tweets\nMexico: 1409 tweets\nSpain: 1464 tweets\nGermany: 1426 tweets\n"
     ]
    }
   ],
   "source": [
    "# Using RDD (3 points)\n",
    "\n",
    "# Map, filter, and reduce to get the total number of tweets per country\n",
    "country_tweet_counts = (\n",
    "    rdd_twitter.map(lambda row: (row[6], 1))  # Map each location to a count of 1\n",
    "    .filter(lambda x: x[0] is not None)     # Filter out rows with null locations (not really necessary)\n",
    "    .reduceByKey(lambda a, b: a + b)        # Reduce by key (country) to sum the counts\n",
    "    .collect()                              # Collect the results\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for country, count in country_tweet_counts:\n",
    "    print(f\"{country}: {count} tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672a3e28-433e-4669-9612-2931ee30563c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|location|count|\n+--------+-----+\n|Germany |1426 |\n|India   |1480 |\n|China   |1457 |\n|Chile   |1410 |\n|Italy   |1422 |\n|Spain   |1464 |\n|USA     |1539 |\n|Mexico  |1409 |\n|UK      |1376 |\n|Canada  |1441 |\n|Pakistan|1470 |\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame (2 points)\n",
    "# Group by country and count the number of tweets\n",
    "tweets_per_country_df = df_twitter.groupBy(\"location\").count()\n",
    "\n",
    "tweets_per_country_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24294129-3f68-4d94-b6e1-627f0f23973d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n|country |total_tweets|\n+--------+------------+\n|Germany |1426        |\n|India   |1480        |\n|China   |1457        |\n|Chile   |1410        |\n|Italy   |1422        |\n|Spain   |1464        |\n|USA     |1539        |\n|Mexico  |1409        |\n|UK      |1376        |\n|Canada  |1441        |\n|Pakistan|1470        |\n+--------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table. (1 point)\n",
    "\n",
    "sql_query = \"\"\"\n",
    "    SELECT location AS country, COUNT(*) AS total_tweets\n",
    "    FROM twitter\n",
    "    WHERE location IS NOT NULL\n",
    "    GROUP BY location\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "tweets_per_country_sql_df = spark.sql(sql_query)\n",
    "\n",
    "tweets_per_country_sql_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8907e096-9205-47b6-b76a-513f08a15b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a90e738-eea4-47d2-b029-629ac1d14f50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1 save the data such that you have seperate folder per country (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3413113-fbdf-477a-8473-5dad5cc94a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using DataFrame\n",
    "\n",
    "# Column `hashtags` has a data type of array<string>, which is not supported by CSV.\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Convert the array of hashtags into a single string with commas\n",
    "df_twitter = df_twitter.withColumn(\"hashtags\", concat_ws(\",\", \"hashtags\"))\n",
    "\n",
    "# divide the df per country ('location') and save it as a .csv\n",
    "df_twitter.write.partitionBy('location').csv(\"locations\", header = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55f27209-6a78-4c31-b50a-cfc91ceea32b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.2 Save the data as parquet files (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e65d90-76a0-4fd8-82ff-58f7258fd49e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using DataFrame\n",
    "df_twitter.write.partitionBy('location').parquet('/FileStore/tables/outputs')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Assignment_2_claudiavalverde",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
