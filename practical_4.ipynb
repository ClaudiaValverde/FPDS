{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Constrained Optimization: Equality Constraints\n","\n","November 2024\n","\n","Joel Dieguez (niub17087652) and Cl√†udia Valverde (niub20441186)"],"metadata":{"id":"rtpCsVjOiYHH"}},{"cell_type":"markdown","source":["**Abstract**\n","\n","This practical focuses on constrained optimization by equality constraints. The method shown here consists of turning a constrained optimization problem into an unconstrained optimization problem using Lagrange multipliers."],"metadata":{"id":"CXz5UORr_UWa"}},{"cell_type":"code","source":["# importing necessary packages\n","import numpy as np\n","from scipy.linalg import solve"],"metadata":{"id":"1NQzaPqzCoRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Proposed Experiments"],"metadata":{"id":"6pJWVzcBi1MD"}},{"cell_type":"markdown","source":["### 1. One simple way to proceed is to take $\\alpha^k = 1$ and iteratively update the current point to obtain the next. This is a simple way to proceed that is proposed to perform first. The stopping condition should be performed over $\\nabla_x L$. Test this approach and check if it works using the starting point proposed in the example"],"metadata":{"id":"SRuBR6dti9vI"}},{"cell_type":"markdown","source":["**Answer**\n","\n","We want to minimize a function $ùëì(ùë•)$, subject to equality constraints $‚Ñé(ùë•)=0$. This can be reformulated into a Lagrangian:\n","$$L(x,Œª)=f(x)‚àíŒªh(x)$$\n","\n","Using the KKT conditions, the problem reduces to solving the system of equations:\n","\n","\n","\n","*   $\\nabla_x L(x, \\lambda) = \\nabla f(x) - \\lambda \\nabla h(x) = 0$\n","\n","*   $h(x) = 0$\n","\n","\n","Here, ùë• and ùúÜ are the variables, and we solve iteratively using a simple gradient-based approach.\n","\n","Let‚Äôs solve the example provided using Sequential Quadratic Programming (SQP).\n","The optimization problem is:\n","\n","$$\n","\\text{minimize } f(x_1, x_2) = e^{3x_1} + e^{-4x_2}\n","$$\n","\n","$$\n","\\text{subject to } h(x_1, x_2) = x_1^2 + x_2^2 - 1 = 0\n","$$\n","\n"],"metadata":{"id":"9-kk_tyQBsn9"}},{"cell_type":"code","source":["# Objective function and its gradient\n","def f(x0,y0):\n","    return np.exp(3 * x0) + np.exp(-4 * y0)\n","\n","def grad_f(x0, y0):\n","    return np.array([3 * np.exp(3 * x0), -4 * np.exp(-4 * y0)])"],"metadata":{"id":"Wc75TRsxi87g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzMjS-IziBtD"},"outputs":[],"source":["# Constraint function and its gradient\n","def h(x0, y0):\n","    # Constraint function\n","    return x0**2 + y0**2 - 1\n","\n","def grad_h(x0, y0):\n","    # Gradient of the constraint\n","    return np.array([2 * x0, 2 * y0])"]},{"cell_type":"code","source":["# Hessians\n","def hessian_f(x0, y0):\n","    return np.array([[9 * np.exp(3 * x0), 0],\n","                     [0, 16 * np.exp(-4 * y0)]])\n","\n","def hessian_h():\n","    return np.array([[2, 0], [0, 2]])"],"metadata":{"id":"ARHkCAuTEE_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lagran_gradx(x0, y0, lambda_0):\n","    # Gradient of the Lagrangian with respect to x and y\n","    return grad_f(x0, y0) - lambda_0 * grad_h(x0, y0)\n","\n","def lagran_hessianx(x0, y0, lambda_0):\n","    # Hessian of the Lagrangian\n","    return hessian_f(x0, y0) - lambda_0 * np.array([[2, 0], [0, 2]])"],"metadata":{"id":"ogOQbWDfE5Cq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequential Quadratic Programming with Newton's Method\n","def Newton_algorithm(x0, y0, lambda_0, alpha, max_iter, tol):\n","    for i in range(max_iter):\n","        # Build the matrix A to solve Ax=b\n","        A = np.zeros((3, 3))\n","        hessian = lagran_hessianx(x0, y0, lambda_0)\n","        gradient_h = grad_h(x0, y0)\n","\n","        # Fill matrix A\n","        A[:2, :2] = hessian  # Top-left: Hessian of Lagrangian\n","        A[2, :2] = -gradient_h  # Bottom-left: -grad_h\n","        A[:2, 2] = -gradient_h  # Top-right: -grad_h\n","\n","        # Build the vector b\n","        b = np.zeros(3)\n","        b[:2] = -lagran_gradx(x0, y0, lambda_0)  # First two entries: -grad_x L\n","        b[2] = h(x0, y0)  # Last entry: constraint value\n","\n","        # Solve the system of equations A * delta = b\n","        try:\n","            delta = np.linalg.solve(A, b)\n","        except np.linalg.LinAlgError:\n","            print(\"Matrix is singular or ill-conditioned. Stopping iterations.\")\n","            break\n","\n","        # Update variables\n","        x0 += alpha * delta[0]\n","        y0 += alpha * delta[1]\n","        lambda_0 += alpha * delta[2]\n","\n","        # Check for convergence\n","        if np.linalg.norm(lagran_gradx(x0, y0, lambda_0)) < tol:\n","            return x0, y0, lambda_0, i\n","\n","    return x0, y0, lambda_0, i\n"],"metadata":{"id":"mM2CFDkYEvtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tol = 1e-1  # Tolerance for stopping criterion\n","\n","# Run the algorithm\n","x_opt, y_opt, lambda_opt, iterations = Newton_algorithm(x0, y0, lambda_0, alpha, max_iter, tol)\n","\n","# Results\n","print('Iterations:', iterations)\n","print(\"Optimal x = {:.4f}\".format(x_opt))\n","print(\"Optimal y = {:.4f}\".format(y_opt))\n","print(\"Optimal lambda = {:.4f}\".format(lambda_opt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lV1XHYR1HBJ-","executionInfo":{"status":"ok","timestamp":1733088257159,"user_tz":-60,"elapsed":223,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"14e88e4f-6c35-485c-a307-5925b12080d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iterations: 1\n","Optimal x = -0.7486\n","Optimal y = 0.6661\n","Optimal lambda = -0.2161\n"]}]},{"cell_type":"code","source":["tol = 1e-3  # Tolerance for stopping criterion\n","\n","# Run the algorithm\n","x_opt, y_opt, lambda_opt, iterations = Newton_algorithm(x0, y0, lambda_0, alpha, max_iter, tol)\n","\n","# Results\n","print('Iterations:', iterations)\n","print(\"Optimal x = {:.4f}\".format(x_opt))\n","print(\"Optimal y = {:.4f}\".format(y_opt))\n","print(\"Optimal lambda = {:.4f}\".format(lambda_opt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipt9lG2VG_LI","executionInfo":{"status":"ok","timestamp":1733088249384,"user_tz":-60,"elapsed":320,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"45b66b5b-e026-46c1-fff7-0f3fa310cfb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iterations: 2\n","Optimal x = -0.7483\n","Optimal y = 0.6633\n","Optimal lambda = -0.2123\n"]}]},{"cell_type":"code","source":["# Initial values\n","x0, y0 = -1.0, 1.0  # Initial guess for x and y\n","lambda_0 = -1.0  # Initial guess for Lagrange multiplier\n","alpha = 1.0  # Step size\n","max_iter = 100  # Maximum number of iterations\n","tol = 1e-6  # Tolerance for stopping criterion\n","\n","# Run the algorithm\n","x_opt, y_opt, lambda_opt, iterations = Newton_algorithm(x0, y0, lambda_0, alpha, max_iter, tol)\n","\n","# Results\n","print('Iterations:', iterations)\n","print(\"Optimal x = {:.4f}\".format(x_opt))\n","print(\"Optimal y = {:.4f}\".format(y_opt))\n","print(\"Optimal lambda = {:.4f}\".format(lambda_opt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhRo18U5EQfL","executionInfo":{"status":"ok","timestamp":1733088234836,"user_tz":-60,"elapsed":308,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"a6efd264-26bd-4c74-bc76-d355951adcc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iterations: 3\n","Optimal x = -0.7483\n","Optimal y = 0.6633\n","Optimal lambda = -0.2123\n"]}]},{"cell_type":"markdown","source":["**Answer**\n","\n","As also seen in the .pdf, the solution of this optimization problem is $(x^*, y^*) = (-0.7483, 0.6633)$ and $\\lambda^* = (-2.123)$\n","\n","We have tested it with different tolerances; with $1e-3$ with only 2 iterations we already reach the optimal solution. With tolerance 0.1 the convergance is reached in 1 iteration but the optimal solution is not exact. On the other hand, with tolerance 1e-6 the exact solution is found but it takes 3 iterations to reach it."],"metadata":{"id":"8nOyckueZC2k"}},{"cell_type":"markdown","source":["### 2. This basic iteration also has drawbacks, leading to a number of vital questions. It is a Newtonlike iteration, and thus may diverge from poor starting points. In our example we have started from a point that is near to the optimal solution. Try to perform some experiments with starting points that are farther away of the optimal solution."],"metadata":{"id":"tgV9RGZDjvTp"}},{"cell_type":"code","source":["# Define the parameters\n","starting_points = [(-1.0, 1.0), (0.0, 1.0), (0.5, 0.5), (-0.9, 0.9), (4,4), (-9, 9), (20, 10), (-50, 20), (50, 100), (100, 90),]  # List of (x0, y0) starting points\n","lambda_0 = -1.0  # Initial guess for Lagrange multiplier\n","alpha = 1.0  # Step size\n","max_iter = 100  # Maximum number of iterations\n","tol = 1e-6  # Tolerance for stopping criterion"],"metadata":{"id":"hFW4Xta1HX66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hardcoding the starting points\n","for idx, (x0, y0) in enumerate(starting_points):\n","      print(f\"\\nStarting Point {idx + 1}: x0 = {x0}, y0 = {y0}, lambda_0 = {lambda_0}\")\n","      x_opt, y_opt, lambda_opt, iterations = Newton_algorithm(x0, y0, lambda_0, alpha, max_iter, tol)\n","      print(f\"Converged to: x = ({x_opt:.3f}, {y_opt:.3f}), lambda = {lambda_opt:.3f} in {iterations} iterations\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ky8J1p2Ejfc","executionInfo":{"status":"ok","timestamp":1733087647362,"user_tz":-60,"elapsed":234,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"2dc814ed-14ce-4f6b-b3d3-dc5c8bf65c9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting Point 1: x0 = -1.0, y0 = 1.0, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 3 iterations\n","\n","Starting Point 2: x0 = 0.0, y0 = 1.0, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 5 iterations\n","\n","Starting Point 3: x0 = 0.5, y0 = 0.5, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 6 iterations\n","\n","Starting Point 4: x0 = -0.9, y0 = 0.9, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 3 iterations\n","\n","Starting Point 5: x0 = 4, y0 = 4, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 22 iterations\n","\n","Starting Point 6: x0 = -9, y0 = 9, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 7 iterations\n","\n","Starting Point 7: x0 = 20, y0 = 10, lambda_0 = -1.0\n","Converged to: x = (0.995, -0.100), lambda = 29.828 in 13 iterations\n","\n","Starting Point 8: x0 = -50, y0 = 20, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 10 iterations\n","\n","Starting Point 9: x0 = 50, y0 = 100, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 27 iterations\n","\n","Starting Point 10: x0 = 100, y0 = 90, lambda_0 = -1.0\n","Converged to: x = (nan, nan), lambda = nan in 99 iterations\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-58-9661c4f97b00>:6: RuntimeWarning: overflow encountered in exp\n","  return np.array([3 * np.exp(3 * x0), -4 * np.exp(-4 * y0)])\n","<ipython-input-60-3a6e6144ed9f>:4: RuntimeWarning: overflow encountered in exp\n","  [0, 16 * np.exp(-4 * y0)]])\n"]}]},{"cell_type":"code","source":["# with random starting points\n","\n","for idx in range(0, 10):\n","      starting_point = np.random.uniform(low=-100, high=100, size = (2,))\n","      x0 = starting_point[0]\n","      y0 = starting_point[1]\n","      print(f\"\\nStarting Point {idx + 1}: x0 = {x0}, y0 = {y0}, lambda_0 = {lambda_0}\")\n","      x_opt, y_opt, lambda_opt, iterations = Newton_algorithm(x0, y0, lambda_0, alpha, max_iter, tol)\n","      print(f\"Converged to: x = ({x_opt:.3f}, {y_opt:.3f}), lambda = {lambda_opt:.3f} in {iterations} iterations\")"],"metadata":{"id":"wzKCC905j3SM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733087658582,"user_tz":-60,"elapsed":10,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"e61e1ae9-e0c9-4923-cba9-532ecfb586f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting Point 1: x0 = -68.81120809591097, y0 = -70.86127807483052, lambda_0 = -1.0\n","Converged to: x = (nan, nan), lambda = nan in 99 iterations\n","\n","Starting Point 2: x0 = 95.1934399991996, y0 = 30.76158379249611, lambda_0 = -1.0\n","Converged to: x = (nan, nan), lambda = nan in 99 iterations\n","\n","Starting Point 3: x0 = -92.77761818085047, y0 = -58.438721767097924, lambda_0 = -1.0\n","Converged to: x = (nan, nan), lambda = nan in 99 iterations\n","\n","Starting Point 4: x0 = -34.885601124500326, y0 = 26.03955484287343, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 9 iterations\n","\n","Starting Point 5: x0 = -66.31257746942586, y0 = -57.546368235343095, lambda_0 = -1.0\n","Converged to: x = (nan, nan), lambda = nan in 99 iterations\n","\n","Starting Point 6: x0 = 48.64535569575099, y0 = -28.65391750228929, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 38 iterations\n","\n","Starting Point 7: x0 = -27.14253945203629, y0 = 3.279978503692547, lambda_0 = -1.0\n","Converged to: x = (-0.748, 0.663), lambda = -0.212 in 8 iterations\n","\n","Starting Point 8: x0 = -95.28363950721203, y0 = -16.861330059934758, lambda_0 = -1.0\n","Converged to: x = (0.014, -1.000), lambda = 109.163 in 27 iterations\n","\n","Starting Point 9: x0 = -91.10113199741139, y0 = -88.34811526172149, lambda_0 = -1.0\n","Converged to: x = (nan, nan), lambda = nan in 99 iterations\n","\n","Starting Point 10: x0 = 51.62359592024433, y0 = 57.41431515920635, lambda_0 = -1.0\n","Converged to: x = (nan, nan), lambda = nan in 99 iterations\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-58-9661c4f97b00>:6: RuntimeWarning: overflow encountered in exp\n","  return np.array([3 * np.exp(3 * x0), -4 * np.exp(-4 * y0)])\n","<ipython-input-60-3a6e6144ed9f>:4: RuntimeWarning: overflow encountered in exp\n","  [0, 16 * np.exp(-4 * y0)]])\n","<ipython-input-60-3a6e6144ed9f>:3: RuntimeWarning: overflow encountered in exp\n","  return np.array([[9 * np.exp(3 * x0), 0],\n"]}]},{"cell_type":"markdown","source":["**Answer:**\n","\n","For experimental propuses, we have tried the algorithm with some hard-coded random starting points to see the behaviour as we incrementaly distenciate ourselves from the original given starting point (-1,1). On the other hand, we have also tried with 10 random starting points using the np.random function to get combination of points that we might not have though of while creating the hard-coded ones.\n","\n","We obeserve that as the distance from the original starting point increases, the more iterations are needed to reach the convergance, in some cases the convergance is not reached in less than 100 iterations while in others we even get an overflow error indicating the values of the points are too high to compute the hessian.\n","\n","Overall this indicates that the Newton Algorithm works best in a local way, as for further points the method does not work."],"metadata":{"id":"jPWbQDV7WuKn"}},{"cell_type":"markdown","source":["### 3. One way to find the optimal solution from points that are far away of the optimal solution is to start the optimization with another function that allows us to find an approximation to the solution we are looking for. Once an approximate solution is found, we can apply the Newton-based technique we have presented previously to find the optimal solution.\n","\n","### The function that allows us to find an approximation to the solution we are looking for is called, in this context, the merit function. Usually, a merit function is the sum of terms that include the objective function and the amount of infeasibility of the constraints. One example of a merit function for the problem we are treating is the quadratic penalty function (i.e. constraints are penalized quadratically)\n","\n","$\n","M(x_1, x_2) = f(x_1, x_2) + œÅh(x_1, x_2)^2\n","$\n","\n","### where $œÅ$ is some positive number. The greater the value of $œÅ$, the greater the penalty for infeasibility. The difficulty arises in defining a proper merit function for a particular equality constrained problem. Here we propose you to take $œÅ = 10$ (thus, we penalize a lot the constraint) and perform a classical gradient descent (with backtraking if you want) to find and approximation to the solution we are looking for. Observe if you arrive near to the optimal solution of the problem.\n","\n","### Take into account that you may have numerical problems with the gradient. A simple way to deal with it is to normalize the gradient at each iteration, $\\nablaŒú(x)/ ||\\nabla M(x)||$, and use this normalized gradient as search direction."],"metadata":{"id":"Qc9gGHpfj7nL"}},{"cell_type":"code","source":["# Merit function and its gradient\n","def merit_function(x, rho):\n","    return f(x[0], x[1]) + rho * h(x[0], x[1])**2\n","\n","def grad_merit_function(x, rho):\n","    grad_f_val = grad_f(x[0], x[1])\n","    grad_h_val = grad_h(x[0], x[1])\n","    h_val = h(x[0], x[1])\n","    return grad_f_val + 2 * rho * h_val * grad_h_val"],"metadata":{"id":"65r3XNXUlM5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient descent for merit function\n","def gradient_descent_merit(x0, rho, alpha, max_iter, tol):\n","    x = np.array(x0, dtype=float)\n","\n","    for i in range(max_iter):\n","        grad_M = grad_merit_function(x, rho)\n","        grad_norm = np.linalg.norm(grad_M)\n","\n","        # Normalized gradient descent step\n","        x_new = x - alpha * grad_M / grad_norm\n","\n","        # Backtracking line search\n","        while merit_function(x_new, rho) >= merit_function(x, rho):\n","            alpha *= 0.5\n","            x_new = x - alpha * grad_M\n","\n","        if abs(merit_function(x_new, rho) - merit_function(x, rho)) < tol or  np.linalg.norm(grad_M/grad_norm) < tol:\n","            print(f\"Converged to: x = ({x[0]:.3f}, {x[1]:.3f}) in {i} iterations\")\n","            return x_new\n","\n","        if i % 10 == 0:  # Optional logging every 10 iterations\n","            print(f\"Iteration {i}: x = ({x[0]:.3f}, {x[1]:.3f}), Gradient Norm = {grad_norm:.3e}\")\n","\n","        else:\n","          x = x_new\n","\n","    print(f\"Did not converge after {max_iter} iterations. Last x = ({x[0]:.3f}, {x[1]:.3f})\")\n","    return x"],"metadata":{"id":"xmIUPdafYIfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test gradient descent with merit function\n","rho = 10\n","alpha = 1\n","max_iter = 1000\n","tol = 1e-3\n","\n","x0 = [-0.5, 10]  # Random Starting point\n","approx_solution = gradient_descent_merit(x0, rho, alpha, max_iter, tol)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvIjDIOJSbrd","executionInfo":{"status":"ok","timestamp":1733091850752,"user_tz":-60,"elapsed":329,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"d71c80ae-4572-4d48-f27a-4f751898b547"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0: x = (-0.500, 10.000), Gradient Norm = 3.975e+04\n","Iteration 10: x = (-0.061, 1.011), Gradient Norm = 2.618e+00\n","Iteration 20: x = (-0.422, 0.913), Gradient Norm = 7.244e-01\n","Iteration 30: x = (-0.545, 0.845), Gradient Norm = 4.178e-01\n","Converged to: x = (-0.623, 0.795) in 37 iterations\n"]}]},{"cell_type":"code","source":["# Test gradient descent with merit function\n","rho = 10\n","alpha = 1\n","max_iter = 1000\n","tol = 1e-3\n","\n","x0 = [-1, 1]  # Change starting point\n","approx_solution = gradient_descent_merit(x0, rho, alpha, max_iter, tol)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mtweNOKYITh","executionInfo":{"status":"ok","timestamp":1733091850752,"user_tz":-60,"elapsed":6,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"49c6ab40-9836-4d25-8f0e-a802f8010dec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0: x = (-1.000, 1.000), Gradient Norm = 5.641e+01\n","Converged to: x = (-0.717, 0.710) in 7 iterations\n"]}]},{"cell_type":"markdown","source":["**Answer**\n","\n","We can see that we are reaching close to the optimal value by changing the starting point and getting help from the Merit function..."],"metadata":{"id":"mwW-L2XwSwK5"}},{"cell_type":"markdown","source":["### 4. As previously commented, the minimizers of the merit function $M(x_1, x_2)$ do not necessarily have to coincide with the minimizers of the constrained problem. Thus, once we ‚Äúsufficiently‚Äù approach the optimal solution we may use the Newton method (with $Œ± = 1$) to find the solution to the problem.\n","\n","### Therefore the algorithm consists in starting with the Merit function to obtain an approximation to the optimal point we are looking for. Once an approximation to the solution is found, use the Newton-based method to find the optimal solution. Check if you are able to find the optimal solution to the problem."],"metadata":{"id":"mcA-_XMDlPJk"}},{"cell_type":"code","source":["print(approx_solution)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0K1VRgKS4Dj","executionInfo":{"status":"ok","timestamp":1733091855682,"user_tz":-60,"elapsed":270,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"f47dc63d-58ff-405a-b661-2656d0514b03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.71570856  0.70752538]\n"]}]},{"cell_type":"code","source":["solution = Newton_algorithm(approx_solution[0], approx_solution[1], -1, 1, 100, 1e-3)"],"metadata":{"id":"R18A5rffTCwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x0, y0, lambda0, iterations = solution\n","\n","print('Iterations to reach optimal solution:', iterations)\n","print('Optimal Solution:', x0, ',', y0)\n","print('Optimal lambda', lambda0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Yl7l1TsTSm9","executionInfo":{"status":"ok","timestamp":1733091858683,"user_tz":-60,"elapsed":221,"user":{"displayName":"Cl√†udia Valverde Sanchez","userId":"17089465752638224867"}},"outputId":"5c7162a1-5264-4ea6-86d4-4a01f379ddec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iterations to reach optimal solution: 2\n","Optimal Solution: -0.7483354358209479 , 0.6633206217919233\n","Optimal lambda -0.21232445155672172\n"]}]},{"cell_type":"markdown","source":["**Answer**\n","\n","Starting with the approx solution found above (with the merit function), we have been able to use Newton to obtain the exact same solution as in the pdf. The method works."],"metadata":{"id":"meXpT4Z6To8-"}}]}